{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample Pairing + TenCrop iCaRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01mnXmFnUFdV"
      },
      "source": [
        "**Import libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crURUb9gURME"
      },
      "source": [
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader,ConcatDataset, random_split\n",
        "from torch.backends import cudnn\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.models import resnet\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Load resnet_cifar.py\n",
        "import os\n",
        "if not os.path.exists(\"./resnet_cifar.py\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=14ugdr3UoIWHmRCRS9KrJiQmCRK9WvCVj' -O resnet_cifar.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US3YGeI1UT4U"
      },
      "source": [
        "**Set Arguments**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn1Q6EXlUbQS"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "K = 2000\n",
        "NUM_EPOCHS = 70 \n",
        "\n",
        "LR = 2.0\n",
        "MOMENTUM = 0.9\n",
        "STEP_SIZE = [49,63]\n",
        "GAMMA = 0.2\n",
        "WEIGHT_DECAY = 1e-5\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "# Random seeds\n",
        "np.random.seed(653)\n",
        "torch.manual_seed(653)\n",
        "torch.cuda.manual_seed(653)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIttza66zgeP"
      },
      "source": [
        "**Some utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ01T0SdzfpZ"
      },
      "source": [
        "def compute_means_of_exemplars(net, dataset, exemplars_idx, n):\n",
        "    \"\"\"\n",
        "    Compute the L2-normalized mean-of-exemplars (AKA average feature vectors) for the first n classes seen\n",
        "    Returns:\n",
        "        a (seen_classes_counter, 64) numpy array whose rows are the mean-of-exemplars of the seen_classes_counter classes seen so far, computed using the current exemplars\n",
        "  \n",
        "    \"\"\"\n",
        "    if n == 0:    # if seen_classes_counter = 0 then return an empty array\n",
        "        return np.array([]).reshape((0,64))\n",
        "    \n",
        "    class_means = []    # this will contain seen_classes_counter lists; i-th list will contain the L2-normalized mean-of-exemplars for the i-th seen class\n",
        "\n",
        "    for i in range(n):   # for each i-th seen class\n",
        "\n",
        "        # Retrieve the exemplars of the i-th seen class\n",
        "        list_exemplars = [dataset[exemplar_idx][0] for exemplar_idx in exemplars_idx[i]]    # a list of tensors, containing the m exemplars of the i-th seen class\n",
        "        exemplars_images = torch.stack(list_exemplars).cuda()     # a tensor containing the m exemplars of the i-th seen class    \n",
        "\n",
        "        # Compute the average feature vector of the i-th seen class\n",
        "        features = net.get_features(exemplars_images).cpu().data.numpy()    # feature vectors of the m exemplars of the i-th seen class (m, 64)\n",
        "        features = features / np.array([np.linalg.norm(features, axis=1)]).T    # L2-normalization of features\n",
        "        class_mean = features.mean(axis=0)  # average feature vector for the i-th seen class (64)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)    # L2 normalization\n",
        "        class_means.append(class_mean)\n",
        "\n",
        "    return np.array(class_means)    # (seen_classes_counter, 64)\n",
        "\n",
        "\n",
        "def show_heatmap_CM(labels, predictions):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix as a heat map, given ground truth labels and the model predictions\n",
        "    \n",
        "    Params:\n",
        "        labels: ground truth labels\n",
        "        predictions: model predictions of the labels\n",
        "\n",
        "    Return:\n",
        "        Show the heatmap\n",
        "        x axis: predicted class\n",
        "        y axis: true class\n",
        "    \n",
        "    TODO put this in utils.py\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(9,9))\n",
        "\n",
        "    # Build confusion matrix (as a 100x100 numpy array)\n",
        "    cm = confusion_matrix(labels, predictions, labels=seen_classes)\n",
        "\n",
        "    # Convert the confusion matrix to a 100x100 pandas dataframe (dimensions len(labels) x len(predictions) )\n",
        "    df_cm = pd.DataFrame(cm, seen_classes, seen_classes)\n",
        "    df_cm.columns = np.arange(100)+20    # for plotting reasons\n",
        "    df_cm.index = np.arange(100)+20    # for plotting reasons\n",
        "\n",
        "    # Visualize the confusion matrix as a heat map\n",
        "    # TODO add xticklabels and yticklabels as in the iCaRL paper: https://stackoverflow.com/a/40740017\n",
        "    ax = sns.heatmap(df_cm, xticklabels=20, yticklabels=20, cbar=False, square=False,cmap=\"OrRd\")\n",
        "    sns.set(font_scale = 2)\n",
        "    ax.set(xlabel='Predicted class', ylabel='True class')\n",
        "    pos, textvals = plt.xticks()\n",
        "    plt.xticks(np.array(pos)+20, textvals, va=\"center\")\n",
        "    pos, textvals = plt.yticks()\n",
        "    plt.yticks(np.array(pos)+20, textvals, va=\"center\")\n",
        "    ax.tick_params(axis='x', pad=15)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def CIFAR100_with_indices(cls):\n",
        "    \"\"\"\n",
        "    Modifies the CIFAR100 class to return a tuple (data, target, index)\n",
        "    instead of just (data, target). index is a relative index.\n",
        "    Ref: https://discuss.pytorch.org/t/how-to-retrieve-the-sample-indices-of-a-mini-batch/7948/19\n",
        "    \"\"\"\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target, index\n",
        "\n",
        "    return type(cls.__name__, (cls,), {'__getitem__': __getitem__,})\n",
        "\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]     # from list of lists to flat list\n",
        "\n",
        "\n",
        "def subset_indices(dataset, classes, exemplars_idx=[], no_exemplars=False):\n",
        "    \"\"\"\n",
        "    Returns the indices for the creation of a subset of a dataset containing only the images of the specified classes AND the exemplars of the previous classes\n",
        "    \"\"\"\n",
        "\n",
        "    indices = []\n",
        "\n",
        "    # Current classes\n",
        "    for _, img_label, img_index in dataset:\n",
        "        if img_label in classes:\n",
        "            indices.append(img_index)     # append the index of those images belonging to class c\n",
        "\n",
        "    # Exemplars of previous classes\n",
        "    flat_list_exemplars_idx = flatten(exemplars_idx)\n",
        "    indices.extend(flat_list_exemplars_idx)\n",
        "\n",
        "    return indices\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuR_5X81gOYl"
      },
      "source": [
        "**Define Sample Pairing class**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRgs1kUpgTws"
      },
      "source": [
        "class SamplePairing(object):\r\n",
        "    def __init__(self, train_data, p):\r\n",
        "        self.train_data = train_data\r\n",
        "        self.pool = self.load_dataset()\r\n",
        "        self.p = p\r\n",
        "\r\n",
        "    def load_dataset(self):\r\n",
        "        dataset_train_raw = self.train_data\r\n",
        "        return dataset_train_raw\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "        toss = np.random.choice([1, 0], p=[self.p, 1 - self.p])\r\n",
        "\r\n",
        "        if toss:\r\n",
        "            img_a_array = np.asarray(img)\r\n",
        "\r\n",
        "            # pick one image from the pool\r\n",
        "            img_b = random.choice(self.pool)\r\n",
        "            img_b_array = np.asarray(img_b[0])\r\n",
        "\r\n",
        "            # mix two images\r\n",
        "            mean_img = np.mean([img_a_array, img_b_array], axis=0)\r\n",
        "            img = Image.fromarray(np.uint8(mean_img))\r\n",
        "            \r\n",
        "            # could have used PIL.Image.blend\r\n",
        "\r\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wkPG3vuUiBK"
      },
      "source": [
        "**Define Data Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omg4hzrgUlfR"
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
        "                                                            std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404]) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "\n",
        "beforeSamplePairing_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                                    transforms.RandomHorizontalFlip(p=0.5)])\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Pad(4, fill=0, padding_mode='constant'),\n",
        "                                     transforms.TenCrop(32),     # TenCrop crops the input image at the four corners and at the center with 32x32 cropping sections\n",
        "                                     transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "                                     transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
        "                                                                                                       std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])(crop) for crop in crops]))])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZYBrIvsUl2M"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5DSZrgzUqFP"
      },
      "source": [
        "# This new version of the CIFAR100 class now returns (img, target, index) when you loop on it with dataloaders\n",
        "CIFAR100 = CIFAR100_with_indices(CIFAR100)\n",
        "\n",
        "# Load CIFAR100 training and test datasets\n",
        "cifar100_training = CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "cifar100_samplePairing = CIFAR100(root='./data', train=True, download=True, transform=beforeSamplePairing_transform)\n",
        "cifar100_test = CIFAR100(root='./data', train=False, transform=eval_transform)\n",
        "cifar100_training_noaugm = CIFAR100(root='./data', train=True, download=True, transform=eval_transform)\n",
        "cifar100_training_sp = cifar100_training\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f'Training set size: {len(cifar100_training)}')\n",
        "print(f'Test set size: {len(cifar100_test)}')\n",
        "\n",
        "\n",
        "# Create an array with a random permutation of the 100 classes, with 10 rows of 10 classes each\n",
        "classes = np.random.permutation(np.arange(100)).reshape((10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ejm8r9xUuDn"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynBoTE5wUzil"
      },
      "source": [
        "from resnet_cifar import resnet32\n",
        "net = resnet32(num_classes = 100)   # Loading ResNet32 model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5DlyxgSUz6Q"
      },
      "source": [
        "**Define loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omEZE_sQU4Cn"
      },
      "source": [
        "# Classification loss + distillation loss\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean')   # but: reduction='sum' is as seen in iCaRL paper (mean is not applied anywhere in the loss)\n",
        "\n",
        "# All the loss are BCEwithLogitsLoss\n",
        "def compute_loss(outputs, one_hot_labels, classes, task_step_counter, criterion = criterion, old_outputs = None):\n",
        "    \"\"\"\n",
        "    Computes the loss presented in the iCaRL paper (classification loss + distillation loss),\n",
        "    which is basically a binary cross-entropy loss, in which the \"true distribution\" vector y in the formula also includes the values of some pre-updated network outputs.\n",
        "    Params:\n",
        "        outputs: (BATCH_SIZE, NUM_CLASSES) matrix of logits of the current batch of images\n",
        "        one_hot_labels: (BATCH_SIZE, NUM_CLASSES) each row is the one-hot-encoded ground-truth label of the images of the current batch.\n",
        "        classes: the array with the random permutation of the 100 classes (10 rows with 10 classes each)\n",
        "        old_outputs: (BATCH_SIZE, NUM_CLASSES) matrix of the pre-updated network outputs of all the nodes (not only those associated with the previously seen t-10 classes), AKA q_i values. It is None when the first set of classes is encountered.\n",
        "        criterion: BCEWithLogitsLoss\n",
        "    Returns:\n",
        "        The fraction of the total loss associated with the current batch of images\n",
        "    \"\"\"\n",
        "\n",
        "    if old_outputs is not None:\n",
        "        sig = nn.Sigmoid()\n",
        "        columns_idx_of_pre_updated_outputs = seen_classes[:-10]     # indices of the columns associated with the q_i values of the previously seen classes\n",
        "        one_hot_labels[:, columns_idx_of_pre_updated_outputs] = sig(old_outputs[:,columns_idx_of_pre_updated_outputs])  # each row of this matrix is now the target \"true\" distribution in the cross-entropy loss to be applied\n",
        "    \n",
        "    loss = criterion(outputs, one_hot_labels)\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_WXopXU4PZ"
      },
      "source": [
        "**Training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poq9MmpnU7Pg"
      },
      "source": [
        "net = net.to(DEVICE)        # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark             # Calling this optimizes runtime\n",
        "\n",
        "test_acc_history = []       # this list shall contain 10 values (one for each seen class)\n",
        "test_acc_history_CNN = []   # this list shall contain 10 values (one for each seen class)\n",
        "training_acc_history = []\n",
        "\n",
        "global_step_counter = 0     # Incremented by one every time a training iteration inside an epoch ends\n",
        "task_step_counter = 1       # Incremented by one every time one set of 10 classes is seen\n",
        "\n",
        "seen_classes_counter = 0    # Incremented by 10 every time one set of 10 classes is seen\n",
        "seen_classes = []           # Is appended with a list of the 10 classes currently seen, each time they are seen\n",
        "\n",
        "old_outputs = None\n",
        "\n",
        "exemplars_idx = [ [] for _ in range(100) ]     # a list of 100 lists which will contain m int values each; the i-th inner list will contain cifar100_training indices of exemplar images for the i-th class seen\n",
        "\n",
        "for current_classes in classes:     # 10 cycles (over the 10 sets of classes)\n",
        "    TEMP_exemplars_idx = deepcopy(exemplars_idx)\n",
        "    training_subset_sp_idx = subset_indices(cifar100_samplePairing, current_classes) \n",
        "\n",
        "    training_subset_sp = Subset(cifar100_samplePairing, training_subset_sp_idx)\n",
        "    sp_train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      SamplePairing(training_subset_sp, p =0.8),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
        "                                                            std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])]) # Normalizes tensor with mean and standard deviation\n",
        "\n",
        "    cifar100_training_augm = CIFAR100(root='./data', train=True, transform=sp_train_transform)\n",
        "    training_subset_idx = subset_indices(cifar100_training_augm, current_classes)  \n",
        "    training_subset = Subset(cifar100_training_augm, training_subset_idx)\n",
        "\n",
        "    augm_training_dataloader = DataLoader(training_subset, num_workers=4, batch_size=BATCH_SIZE, drop_last=True, shuffle = True)\n",
        "\n",
        "    print('training dataset:',len(training_subset))\n",
        "\n",
        "    if seen_classes_counter>1:\n",
        "        augm_training_datasets = []\n",
        "        augm_eval_datasets = []\n",
        "\n",
        "        training_subset_sp_idx = subset_indices(cifar100_samplePairing, current_classes) \n",
        "        \n",
        "        training_subset_sp = Subset(cifar100_samplePairing, training_subset_sp_idx)\n",
        "        ex_training_subset_sp = Subset(cifar100_samplePairing, flatten(exemplars_idx))\n",
        "        augm_training_subset_sp = ConcatDataset([training_subset_sp,ex_training_subset_sp])\n",
        "\n",
        "        sp_train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                      SamplePairing(augm_training_subset_sp, p=0.8),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
        "                                                            std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])]) # Normalizes tensor with mean and standard deviation\n",
        "                                                 \n",
        "        cifar100_training_sp = CIFAR100(root='./data', train=True, transform=sp_train_transform)\n",
        "                                                 \n",
        "        training_subset_sp = Subset(cifar100_training_sp, training_subset_sp_idx)\n",
        "        ex_training_subset_sp = Subset(cifar100_training_sp, flatten(exemplars_idx))\n",
        "\n",
        "        augm_exemplars_training_dataset = ConcatDataset([ex_training_subset_sp,ex_training_subset_sp])\n",
        "        augm_exemplars_training_subset, _ = random_split(augm_exemplars_training_dataset,[3000,len(augm_exemplars_training_dataset)-3000])\n",
        "\n",
        "\n",
        "        training_datasets = [training_subset_sp, ex_training_subset_sp, augm_exemplars_training_subset]\n",
        "\n",
        "        augm_training_dataset = ConcatDataset(training_datasets)\n",
        "    \n",
        "        augm_training_dataloader = DataLoader(augm_training_dataset, shuffle=True, num_workers=4, batch_size=BATCH_SIZE, drop_last=True)\n",
        "    \n",
        "    \n",
        "    # Initialize the optimizer and the scheduler\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "\n",
        "    #################### UPDATE REPRESENTATION ####################\n",
        "    # train on the next set of 10 unseen classes\n",
        "\n",
        "\n",
        "    \n",
        "    # 10 previously unseen classes are now being seen\n",
        "    seen_classes_counter += 10\n",
        "    seen_classes += current_classes.tolist()\n",
        "\n",
        "    epoch_step_counter = 0      # Incremented by one each time an epochs end\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'Starting epoch {epoch+1}/{NUM_EPOCHS}, LR = {scheduler.get_last_lr()}')\n",
        "        t = time.time()\n",
        "\n",
        "        net.train(True)\n",
        "        \n",
        "        total_training_corrects = 0\n",
        "        running_loss = 0\n",
        "\n",
        "        for images, labels, _ in augm_training_dataloader:\n",
        "\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            # If this is not the first time classes are seen, compute the output logits of the network obtained previously\n",
        "            if task_step_counter >= 2:\n",
        "                old_net = torch.load('resNet_task' + str(task_step_counter - 1) + '.pt').train(False)\n",
        "                old_outputs = old_net(images)\n",
        "            \n",
        "            # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "\n",
        "            # Compute loss\n",
        "            one_hot_labels = torch.eye(100)[labels].to(DEVICE)\n",
        "            loss = compute_loss(outputs, one_hot_labels,classes,task_step_counter, criterion, old_outputs)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the running amount of correct predictions\n",
        "            total_training_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if epoch_step_counter % LOG_FREQUENCY == 0:\n",
        "                print(f'Step: {epoch_step_counter}, training loss: {loss.item()}')\n",
        " \n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            # Update the local and global step counter\n",
        "            epoch_step_counter += 1\n",
        "            global_step_counter += 1\n",
        "\n",
        "        # Compute the accuracy on the current 10 classes (only for reporting purposes)\n",
        "        current_classes_training_accuracy = total_training_corrects / (float(len(augm_training_dataloader)) * BATCH_SIZE)\n",
        "        print(f'------ Epoch {epoch+1}/{NUM_EPOCHS} of the training on the {task_step_counter}° set of classes has ended')\n",
        "        print(f'------ Training accuracy (only on the current 10 classes): {current_classes_training_accuracy}\\n------ Elapsed time for this epoch: {time.time() - t}')\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Save the\n",
        "    torch.save(net, f'resNet_task{task_step_counter}.pt')\n",
        "\n",
        "\n",
        "\n",
        "    #################### EXEMPLARS MANAGEMENT ####################\n",
        "    # exemplar management part\n",
        "\n",
        "\n",
        "\n",
        "    ###### Reduce exemplar sets of previously seen classes\n",
        "\n",
        "    m = int(np.ceil(K / float(seen_classes_counter)))   # NB: this can result in a number of exemplars slightly larger than K, but that's how it's done in the original iCaRL code (see main_resnet_tf.py)\n",
        "    for i in range(seen_classes_counter - 10):\n",
        "        exemplars_idx[i] = exemplars_idx[i][:m]\n",
        "    \n",
        "    ###### Construct exemplar set for each of the current 10 classes seen\n",
        "\n",
        "    training_subset_idx = subset_indices(cifar100_training_sp, current_classes)   # list of 5000 indices\n",
        "    training_subset = Subset(cifar100_training_sp, training_subset_idx)    # subset of all training images which label is in current_classes\n",
        "\n",
        "    current_classes_means = []  # at the end of the next for loop, this will be (10,64)\n",
        "\n",
        "    for i, cls in enumerate(current_classes): # for each current class\n",
        "\n",
        "        # Retrieve the 500 images of class cls\n",
        "        class_subset_idx = subset_indices(training_subset, [cls])   # absolute indices of the 500 images of class cls\n",
        "        class_subset_images = torch.stack([cifar100_training_sp[j][0] for j in class_subset_idx]).cuda()  # tensor of the 500 images of class cls\n",
        "\n",
        "        # Compute the (exact) class mean (AKA mean feature vector) μ of class cls\n",
        "        features = net.get_features(class_subset_images).cpu().data.numpy()     # (len(class_subset), net.fc.in_features) = (500, 64)-d matrix\n",
        "        features = features / np.array([np.linalg.norm(features, axis=1)]).T    # normalize features (the paper mentions an L2-normalization of features)\n",
        "        class_mean = features.mean(axis=0)  # current class mean of features (64)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)    # L2 normalization x / ||x||\n",
        "        current_classes_means.append(class_mean)\n",
        "\n",
        "        # Construct exemplar set for class cls\n",
        "        local_exemplars_idx = []    # relative indices of the 500 images of class cls\n",
        "        for k in range(500):  # m; putting 500 here is a TRICK: all the 500 currently seen images will be used to compute the currently seen classes means, not the m exemplars, so they will be the exact class means\n",
        "            temp = features[local_exemplars_idx]    # features of the k exemplars already found\n",
        "            temp = np.sum(temp, axis=0)             #\n",
        "            temp = features + temp                  # broadcasting! (500, 64)\n",
        "            temp = temp / float(k+1)                # a tiny broadcasting, (500, 64)\n",
        "            temp = class_mean - temp                # broadcasting! (500, 64)\n",
        "            temp = np.linalg.norm(temp, axis=1)     # final norm, (500)\n",
        "            argsort = np.argsort(temp)\n",
        "\n",
        "            for j in argsort:\n",
        "                if j not in local_exemplars_idx:\n",
        "                    exemplars_idx[(seen_classes_counter-10) + i].append(class_subset_idx[j])    # append the absolute index of the exemplar p_k to the exemplar set P_i of the i-th class\n",
        "                    local_exemplars_idx.append(j)\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "    #################### TEST PHASE ####################\n",
        "    # NMoE classifier (see paper for mathematical details on the algorithm)\n",
        "\n",
        "    # Create a dataloader for the test images of all (and only) the classes seen so far\n",
        "    test_subset_idx = subset_indices(cifar100_test, seen_classes)\n",
        "    test_subset = Subset(cifar100_test, test_subset_idx)\n",
        "    test_dataloader = DataLoader(test_subset, shuffle=False, num_workers=4, batch_size=BATCH_SIZE)\n",
        "\n",
        "    net.train(False)    # Set Network to evaluation mode\n",
        "\n",
        "    # Compute the L2-normalized means-of-exemplars for the previously seen (seen_classes_counter-10) classes, and the L2-normalized (exact) class means for the currently seen 10 classes (TRICK)\n",
        "    class_means = compute_means_of_exemplars(net, cifar100_training, exemplars_idx, seen_classes_counter)    # (seen_classes_counter, 64); USE ONLY WITH TRICK: means of exemplars of the previously seen classes and (exact) class means of the currently seen classes\n",
        "    \n",
        "    \n",
        "    all_test_labels = torch.LongTensor([]).to(DEVICE)\n",
        "    all_test_preds = torch.LongTensor([]).to(DEVICE)\n",
        "\n",
        "    total_test_corrects = 0\n",
        "    total_training_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in tqdm(test_dataloader):    # images is a (128,10,3,32,32) tensor, labels is a (128) tensor\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            bs, ncrops, c, h, w = images.size()\n",
        "\n",
        "\n",
        "\n",
        "            ##### CLASSIFICATION WITH NME\n",
        "            # Compute the L2-normalized feature vectors of the images of the current batch of test images\n",
        "            features = net.get_features(images.view(-1, c, h, w)).cpu().data.numpy()    # (1280, 64)-d matrix; each row is a ϕ(x)\n",
        "            features = features / np.array([np.linalg.norm(features, axis=1)]).T    # L2 normalization of the ϕ(x)'s\n",
        "            #features = features.view(bs, ncrops, -1).mean(axis=1)   # (128,10,64)\n",
        "            \n",
        "            # Get predictions with the NMoE classifier\n",
        "            preds = []\n",
        "            for feature in features:\n",
        "                pred_idx = np.argmin( np.linalg.norm(feature - class_means, axis=1) )    # relative index of the closest mean of exemplars; this indices a list in the list of lists exemplars_idx\n",
        "                pred = cifar100_training[ exemplars_idx[pred_idx][0] ][1]   # retrieve the label associated with pred_idx; note that all the exemplars which indices are in the same list of exemplars_idx have the same label by definition\n",
        "                preds.append(pred)\n",
        "            preds = torch.LongTensor(preds).view(bs, ncrops)    # from (1280) to (128,10)\n",
        "            # Hard major voting\n",
        "            class_counts = torch.stack([torch.bincount(row, minlength=100) for row in preds])   # (128,100)\n",
        "            preds = torch.argmax(class_counts, dim=1)   # (128)\n",
        "\n",
        "            preds = preds.to(DEVICE)     # to tensor (needed for the next line of code)\n",
        "            current_batch_corrects = torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "\n",
        "            # Update the running statistics (for plotting purposes)\n",
        "            total_test_corrects += current_batch_corrects\n",
        "\n",
        "            # Store the current batch labels and preds; needed later for plotting the heatmap\n",
        "            all_test_labels = torch.cat((all_test_labels, labels), 0)\n",
        "            all_test_preds = torch.cat((all_test_preds, preds), 0)\n",
        "\n",
        "\n",
        "        test_accuracy = total_test_corrects / float(len(test_subset))\n",
        "        test_acc_history.append(test_accuracy)   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### CLASSIFICATION WITH NME OF THE 7000 TRAINING IMAGES (without augmentation and with tencrop)\n",
        "    augm_training_noaugm_subset_idx = subset_indices(cifar100_training_noaugm, current_classes, TEMP_exemplars_idx)   # augm stands for augmented with exemplars of previous classes\n",
        "    augm_training_noaugm_subset = Subset(cifar100_training_noaugm, augm_training_noaugm_subset_idx)\n",
        "    augm_training_noaugm_dataloader = DataLoader(augm_training_noaugm_subset, shuffle=False, num_workers=4, batch_size=BATCH_SIZE, drop_last=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in tqdm(augm_training_noaugm_dataloader):\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            bs, ncrops, c, h, w = images.size()\n",
        "\n",
        "            # Compute the L2-normalized feature vectors of the images of the current batch of test images\n",
        "            features = net.get_features(images.view(-1, c, h, w)).cpu().data.numpy()    # (1280, 64)-d matrix; each row is a ϕ(x)\n",
        "            features = features / np.array([np.linalg.norm(features, axis=1)]).T    # L2 normalization of the ϕ(x)'s\n",
        "            #features = features.view(bs, ncrops, -1).mean(axis=1)   # (128,10,64)\n",
        "            \n",
        "            # Get predictions with the NMoE classifier\n",
        "            preds = []\n",
        "            for feature in features:\n",
        "                pred_idx = np.argmin( np.linalg.norm(feature - class_means, axis=1) )    # relative index of the closest mean of exemplars; this indices a list in the list of lists exemplars_idx\n",
        "                pred = cifar100_training[ exemplars_idx[pred_idx][0] ][1]   # retrieve the label associated with pred_idx; note that all the exemplars which indices are in the same list of exemplars_idx have the same label by definition\n",
        "                preds.append(pred)\n",
        "            preds = torch.LongTensor(preds).view(bs, ncrops)    # from (1280) to (128,10)\n",
        "            # Hard major voting\n",
        "            class_counts = torch.stack([torch.bincount(row, minlength=100) for row in preds])   # (128,100)\n",
        "            preds = torch.argmax(class_counts, dim=1)   # (128)\n",
        "\n",
        "            preds = preds.to(DEVICE)     # to tensor (needed for the next line of code)\n",
        "            current_batch_corrects = torch.sum(preds == labels.data).data.item()\n",
        "            total_training_corrects += current_batch_corrects\n",
        "        training_accuracy = total_training_corrects / float(len(augm_training_noaugm_subset))\n",
        "        training_acc_history.append(training_accuracy)\n",
        "    \n",
        "\n",
        "    ### End of training on the 10 currently seen classes\n",
        "\n",
        "\n",
        "    print(f'--------------- Training ended on the {int(seen_classes_counter / 10)}° set of classes')\n",
        "    print(f'--------------- Test accuracy (NME): {test_accuracy}')\n",
        "    print(f'--------------- Training accuracy (NME): {training_accuracy}')\n",
        "\n",
        "\n",
        "    ###### Reduce exemplar sets of currently seen classes (this is because of the trick)\n",
        "    for i in range( seen_classes_counter-10, seen_classes_counter ):\n",
        "        exemplars_idx[i] = exemplars_idx[i][:m]\n",
        "\n",
        "    # 10 new classes are to be seen next: update the counter\n",
        "    task_step_counter += 1\n",
        "\n",
        "\n",
        "torch.save(net, 'final_net')  # save the final net on Colab's file system\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sORKQtDcKGNt"
      },
      "source": [
        "# Print the ten test accuracies obtained\n",
        "print(test_acc_history)\n",
        "print(test_acc_history_CNN)\n",
        "print(training_acc_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xjbReTSV2s0"
      },
      "source": [
        "**Heatmap Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUD5NbMvnSMo"
      },
      "source": [
        "# Show the confusion matrix for the test set as a heatmap\n",
        "show_heatmap_CM(all_test_labels.cpu(), all_test_preds.cpu())\n",
        "\n",
        "# x axis: predicted class\n",
        "# y axis: true class\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}