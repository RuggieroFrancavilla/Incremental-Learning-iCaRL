{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lwf_baseline_Official.ipynb","provenance":[{"file_id":"1uaYPA7llUMDZ0awD3_q1o9eGKcrWGf51","timestamp":1602349857695}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"01mnXmFnUFdV"},"source":["**Import libraries**\n"]},{"cell_type":"code","metadata":{"id":"crURUb9gURME"},"source":["import time\n","from copy import deepcopy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Subset, DataLoader\n","from torch.backends import cudnn\n","from torch.nn.functional import one_hot\n","\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR100\n","from torchvision.models import resnet\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import seaborn as sns\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","# Load resnet_cifar.py\n","import os\n","if not os.path.exists(\"./resnet_cifar.py\"):\n","    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=14ugdr3UoIWHmRCRS9KrJiQmCRK9WvCVj' -O resnet_cifar.py\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"US3YGeI1UT4U"},"source":["**Set Arguments**\n"]},{"cell_type":"code","metadata":{"id":"Rn1Q6EXlUbQS"},"source":["DEVICE = 'cuda'\n","\n","BATCH_SIZE = 128\n","\n","K = 2000\n","NUM_EPOCHS = 70\n","\n","LR = 2.0\n","MOMENTUM = 0.9\n","STEP_SIZE = [49,63]\n","GAMMA = 0.2\n","WEIGHT_DECAY = 1e-5\n","LOG_FREQUENCY = 10\n","\n","# Random seeds\n","np.random.seed(653)\n","torch.manual_seed(653)\n","torch.cuda.manual_seed(653)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12N6vQQVlXTS"},"source":["**Some utility functions**"]},{"cell_type":"code","metadata":{"id":"UyUKDcXdlZg_"},"source":["def subset_indices(dataset, classes):\r\n","    \"\"\"\r\n","    Returns the indices for the creation of a subset of a dataset containing only the images of the specified classes\r\n","    \"\"\"\r\n","    indices = []\r\n","    for img_index, (_, img_label) in enumerate(dataset):\r\n","        if img_label in classes:\r\n","            indices.append(img_index)     # append the index of those images belonging to class c\r\n","    return indices\r\n","\r\n","\r\n","def show_heatmap_CM(labels, predictions):\r\n","    \"\"\"\r\n","    Plot the confusion matrix as a heat map, given ground truth labels and the model predictions\r\n","    \r\n","    Params:\r\n","        labels: ground truth labels\r\n","        predictions: model predictions of the labels\r\n","\r\n","    Return:\r\n","        Show the heatmap\r\n","        x axis: predicted class\r\n","        y axis: true class\r\n","    \"\"\"\r\n","    fig, ax = plt.subplots(figsize=(9,9))\r\n","\r\n","    # Build confusion matrix (as a 100x100 numpy array)\r\n","    cm = confusion_matrix(labels, predictions, labels=seen_classes)\r\n","\r\n","    # Convert the confusion matrix to a 100x100 pandas dataframe (dimensions len(labels) x len(predictions) )\r\n","    df_cm = pd.DataFrame(cm, seen_classes, seen_classes)\r\n","    df_cm.columns = np.arange(100)+20    # for plotting reasons\r\n","    df_cm.index = np.arange(100)+20    # for plotting reasons\r\n","\r\n","    # Visualize the confusion matrix as a heat map\r\n","    ax = sns.heatmap(df_cm, xticklabels=20, yticklabels=20, cbar=False, square=False, cmap='OrRd')\r\n","    sns.set(font_scale = 2)\r\n","    ax.set(xlabel='Predicted class', ylabel='True class')\r\n","    pos, textvals = plt.xticks()\r\n","    plt.xticks(np.array(pos)+20, textvals, va=\"center\")\r\n","    pos, textvals = plt.yticks()\r\n","    plt.yticks(np.array(pos)+20, textvals, va=\"center\")\r\n","    ax.tick_params(axis='x', pad=15)\r\n","\r\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-wkPG3vuUiBK"},"source":["**Define Data Processing**"]},{"cell_type":"code","metadata":{"id":"omg4hzrgUlfR"},"source":["# Define transforms for training phase\n","train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n","                                      transforms.RandomHorizontalFlip(p=0.5),\n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n","                                                            std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404]) # Normalizes tensor with mean and standard deviation\n","])\n","\n","# Define transforms for the evaluation phase\n","eval_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize(mean=[0.5088964127604166, 0.48739301317401956, 0.44194221124387256],\n","                                                          std=[0.2682515741720801, 0.2573637364478126, 0.2770957707973042])                                    \n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZYBrIvsUl2M"},"source":["**Prepare Dataset**"]},{"cell_type":"code","metadata":{"id":"u5DSZrgzUqFP"},"source":["# Load CIFAR100 training and test datasets \n","cifar100_training = CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n","cifar100_test = CIFAR100(root='./data', train=False, transform=eval_transform)\n","\n","# Check dataset sizes\n","print(f'Training set size: {len(cifar100_training)}')\n","print(f'Test set size: {len(cifar100_test)}')\n","\n","\n","# Create an array with a random permutation of the 100 classes, with 10 rows of 10 classes each\n","classes = np.random.permutation(np.arange(100)).reshape((10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Ejm8r9xUuDn"},"source":["**Prepare Network**"]},{"cell_type":"code","metadata":{"id":"ynBoTE5wUzil"},"source":["from resnet_cifar import resnet32\n","net = resnet32(num_classes = 100)   # Loading ResNet32 model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5DlyxgSUz6Q"},"source":["**Define loss function**"]},{"cell_type":"code","metadata":{"id":"EL1V5uq3mFdS"},"source":["# Classification loss + distillation loss\r\n","criterion = nn.BCEWithLogitsLoss(reduction='mean')   # but: reduction='sum' is as seen in iCaRL paper (mean is not applied anywhere in the loss)\r\n","\r\n","# All the loss are BCEwithLogitsLoss\r\n","def compute_loss(outputs, one_hot_labels, classes, task_step_counter, criterion = criterion, old_outputs = None):\r\n","    \"\"\"\r\n","    Computes the loss presented in the iCaRL paper (classification loss + distillation loss),\r\n","    which is basically a binary cross-entropy loss, in which the \"true distribution\" vector y in the formula also includes the values of some pre-updated network outputs.\r\n","    Params:\r\n","        outputs: (BATCH_SIZE, NUM_CLASSES) matrix of logits of the current batch of images\r\n","        one_hot_labels: (BATCH_SIZE, NUM_CLASSES) each row is the one-hot-encoded ground-truth label of the images of the current batch.\r\n","        classes: the array with the random permutation of the 100 classes (10 rows with 10 classes each)\r\n","        old_outputs: (BATCH_SIZE, NUM_CLASSES) matrix of the pre-updated network outputs of all the nodes (not only those associated with the previously seen t-10 classes), AKA q_i values. It is None when the first set of classes is encountered.\r\n","        criterion: BCEWithLogitsLoss\r\n","    Returns:\r\n","        The fraction of the total loss associated with the current batch of images\r\n","    \"\"\"\r\n","\r\n","    if old_outputs is not None:\r\n","        sig = nn.Sigmoid()\r\n","        columns_idx_of_pre_updated_outputs = seen_classes[:-10]     # indices of the columns associated with the q_i values of the previously seen classes\r\n","        one_hot_labels[:, columns_idx_of_pre_updated_outputs] = sig(old_outputs[:,columns_idx_of_pre_updated_outputs])  # each row of this matrix is now the target \"true\" distribution in the cross-entropy loss to be applied\r\n","    \r\n","    loss = criterion(outputs, one_hot_labels)\r\n","\r\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73_WXopXU4PZ"},"source":["**Training and testing**"]},{"cell_type":"code","metadata":{"id":"poq9MmpnU7Pg"},"source":["# By default, everything is loaded to cpu\n","net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","cudnn.benchmark # Calling this optimizes runtime\n","\n","test_acc_history = []  # this list shall contain 10 values (one for each seen class)\n","\n","global_step_counter = 0\n","task_step_counter = 1 # Incremented by one every time one set of 10 classes is seen\n","\n","seen_classes_counter = 0\n","seen_classes = []\n","old_outputs = None\n","\n","for current_classes in classes:     # 10 cycles (over the 10 sets of classes)\n","\n","    # Create a dataloader for the new 10 classes to train on\n","    training_subset_idx = subset_indices(cifar100_training, current_classes)\n","    training_subset = Subset(cifar100_training, training_subset_idx)\n","    training_dataloader = DataLoader(training_subset, shuffle=True, num_workers=4, batch_size=BATCH_SIZE, drop_last=True)\n","    \n","    # Initialize the optimizer and the scheduler\n","    parameters_to_optimize = net.parameters()\n","    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, STEP_SIZE, gamma=GAMMA)\n","\n","\n","\n","    # TRAINING PHASE\n","    # train on the next set of 10 unseen classes\n","\n","\n","    \n","    # 10 previously unseen classes are now being seen\n","    seen_classes_counter += 10\n","    seen_classes += current_classes.tolist()\n","\n","    epoch_step_counter = 0\n","    pre_update_outputs_already_computed = False\n","\n","    for epoch in range(NUM_EPOCHS):\n","        print(f'Starting epoch {epoch+1}/{NUM_EPOCHS}, LR = {scheduler.get_last_lr()}')\n","        t = time.time()\n","\n","        net.train(True)\n","        \n","        total_training_corrects = 0\n","        running_loss = 0\n","    \n","        for images, labels in training_dataloader:\n","\n","            # Bring data over the device of choice\n","            images = images.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            optimizer.zero_grad() # Zero-ing the gradients\n","\n","            if(task_step_counter >= 2):\n","                old_net = torch.load('resNet_task' + str(task_step_counter - 1) + '.pt').train(False)\n","                old_outputs = old_net(images)\n","            \n","            # Forward pass to the network\n","            outputs = net(images)\n","    \n","            one_hot_labels = torch.eye(100)[labels].to(DEVICE)\n","\n","            loss = compute_loss(outputs, one_hot_labels,classes,task_step_counter, criterion, old_outputs)\n","            \n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update Corrects\n","            total_training_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            # Compute loss based on output and ground truth\n","            one_hot_labels = torch.eye(100)[labels].to(DEVICE)\n","            loss = compute_loss(outputs, one_hot_labels, classes, task_step_counter, criterion, old_outputs)\n","\n","            # Log loss\n","            if epoch_step_counter % LOG_FREQUENCY == 0:\n","                print(f'Step: {epoch_step_counter}, training loss: {loss.item()}')\n"," \n","            # Compute gradients for each layer and update weights\n","            loss.backward()  # backward pass: computes gradients\n","            optimizer.step() # update weights based on accumulated gradients\n","\n","            # Update counters\n","            epoch_step_counter += 1\n","            global_step_counter += 1\n","\n","\n","        current_classes_training_accuracy = total_training_corrects / (float(len(training_dataloader)) * BATCH_SIZE)\n","        print(f'------ Epoch {epoch+1}/{NUM_EPOCHS} of the training on the {task_step_counter}° set of classes has ended.\\n------ Training accuracy (only on the current 10 classes): {current_classes_training_accuracy}\\n------ Elapsed time for this epoch: {time.time() - t}')\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","    \n","    torch.save(net, 'resNet_task{0}.pt'.format(task_step_counter))\n","\n","    task_step_counter += 1\n","\n","\n","\n","    # TEST PHASE\n","    # evaluate the network on (all) the test images of the classes seen so far\n","\n","\n","\n","    # Create a dataloader for the test images of all the classes seen so far\n","    test_subset_idx = subset_indices(cifar100_test, seen_classes)\n","    test_subset = Subset(cifar100_test, test_subset_idx)\n","    test_dataloader = DataLoader(test_subset, shuffle=False, num_workers=4, batch_size=BATCH_SIZE)\n","\n","    net.train(False)    # Set Network to evaluation mode\n","\n","    all_test_labels = torch.LongTensor([]).to(DEVICE)\n","    all_test_preds = torch.LongTensor([]).to(DEVICE)\n","\n","    total_test_corrects = 0\n","\n","    with torch.no_grad():\n","        for images, labels in test_dataloader:\n","            images = images.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            # Forward Pass\n","            outputs = net(images)\n","\n","            # Get predictions (for reporting purposes)\n","            _, preds = torch.max(outputs.data, 1)\n","            current_batch_corrects = torch.sum(preds == labels.data).data.item()\n","\n","            # Update the running amount of correct predictions (for plotting purposes)\n","            total_test_corrects += current_batch_corrects\n","\n","            # Store the current batch labels and preds; needed later for plotting the heatmap\n","            all_test_labels = torch.cat((all_test_labels, labels), 0)\n","            all_test_preds = torch.cat((all_test_preds, preds), 0)\n","    \n","        test_accuracy = total_test_corrects / float(len(test_subset))\n","        test_acc_history.append(test_accuracy)\n","\n","        print(f'---------- Training ended on the {int(seen_classes_counter / 10)}° set of classes\\n---------- Test accuracy: {test_accuracy}')\n","\n","\n","torch.save(net, './SavedNet') # SavedNet on drive folder\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sORKQtDcKGNt"},"source":["print(test_acc_history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xjbReTSV2s0"},"source":["**Heatmap Confusion Matrix**"]},{"cell_type":"code","metadata":{"id":"S0OhHhMXsN4e"},"source":["# Show the confusion matrix for the test set as a heatmap\r\n","show_heatmap_CM(all_test_labels.cpu(), all_test_preds.cpu())"],"execution_count":null,"outputs":[]}]}